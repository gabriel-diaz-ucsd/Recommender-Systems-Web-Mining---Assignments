{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "f = gzip.open(\"young_adult_20000.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 'dc3763cdb9b2cae805882878eebb6a32', 'book_id': '18471619', 'review_id': '66b2ba840f9bd36d6d27f46136fe4772', 'rating': 3, 'review_text': 'Sherlock Holmes and the Vampires of London \\n Release Date: April 2014 \\n Publisher: Darkhorse Comics \\n Story by: Sylvain Cordurie \\n Art by: Laci \\n Colors by: Axel Gonzabo \\n Cover by: Jean Sebastien Rossbach \\n ISDN: 9781616552664 \\n MSRP: $17.99 Hardcover \\n \"Sherlock Holmes died fighting Professor Moriarty in the Reichenbach Falls. \\n At least, that\\'s what the press claims. \\n However, Holmes is alive and well and taking advantage of his presumed death to travel the globe. \\n Unfortunately, Holmes\\'s plans are thwarted when a plague of vampirism haunts Britain. \\n This book collects Sherlock Holmes and the Vampires of London Volumes 1 and 2, originally created by French publisher Soleil.\" - Darkhorse Comics \\n When I received this copy of \"Sherlock Holmes and the Vampires of London\" I was Ecstatic! The cover art was awesome and it was about two of my favorite things, Sherlock Holmes and Vampires. I couldn\\'t wait to dive into this! \\n Unfortunately, that is where my excitement ended. The story takes place a month after Sherlock Holmes supposed death in his battle with Professor Moriarty. Sherlock\\'s plan to stay hidden and out of site are ruined when on a trip with his brother Mycroft, they stumble on the presence of vampires. That is about as much of Sherlock\\'s character that comes through the book. I can\\'t even tell you the story really because nothing and I mean nothing stuck with me after reading it. I never, ever got the sense of Sherlock Holmes anywhere in this graphic novel, nor any real sense of mystery or crime. It was just Sherlock somehow battling vampires that should have had absolutely no trouble snuffing him out in a fight, but somehow always surviving and holding his own against supernatural, super powerful, blazingly fast creatures. \\n The cover art is awesome and it truly made me excited to read this but everything else feel completely flat for me. I tried telling myself that \"it\\'s a graphic novel, it would be hard to translate mystery, details, emotion\" but then I remembered reading DC Comic\\'s \"Identity Crisis\" and realized that was a load of crap. I know it\\'s unfair to compare the two as \"Identity Crisis\" had popular mystery author Brad Meltzer writing it right? Yeah....no. The standard was set that day and there is more than enough talent out there to create a great story in a graphic novel. \\n That being said, it wasn\\'t a horrible story, it just didn\\'t grip me for feel anything like Sherlock Holmes to me. It was easy enough to follow but I felt no sense of tension, stakes or compassion for any of the characters. \\n As far as the vampires go, it\\'s hard to know what to expect anymore as there are so many different versions these days. This was the more classic version which I personally prefer, but again I didn\\'t find anything that portrayed their dominance, calm confidence or sexuality. There was definitely a presence of their physical prowess but somehow that was lost on me as easily as Sherlock was able to defend himself. I know it, wouldn\\'t do to kill of the main character, but this would have a been a great opportunity to build around the experience and beguiling nature of a vampire that had lived so many years of experience. Another chance to showcase Sherlock\\'s intellect in a battle of wits over strength in something more suitable for this sort of story as apposed to trying to make it feel like an action movie. \\n Maybe I expected to much and hoped to have at least a gripping premise or some sort of interesting plot or mystery but I didn\\'t find it here. This may be a must have for serious Sherlock Holmes fans that have to collect everything about him, but if you are looking for a great story inside a graphic novel, I would have to say pass on this one. \\n That artwork is good, cover is great, story is lacking so I am giving it 2.5 out of 5 stars.', 'date_added': 'Thu Dec 05 10:44:25 -0800 2013', 'date_updated': 'Thu Dec 05 10:45:15 -0800 2013', 'read_at': 'Tue Nov 05 00:00:00 -0800 2013', 'started_at': '', 'n_votes': 0, 'n_comments': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = dataset[:10000]\n",
    "testSet = dataset[10000:]\n",
    "\n",
    "trainReviewText = [d['review_text'] for d in trainSet]\n",
    "#print(trainSet[0])\n",
    "#print(trainReviewText[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniCount = defaultdict(int)\n",
    "biCount = defaultdict(int)\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "for review in trainReviewText:\n",
    "    r = ''.join([c for c in review.lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w_u in ws:\n",
    "        uniCount[w_u] += 1\n",
    "\n",
    "    for w_b in ws2:\n",
    "        biCount[w_b] += 1\n",
    "\n",
    "    for w in ws + ws2:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "uCounts = [(uniCount[w], w) for w in uniCount]\n",
    "uCounts.sort()\n",
    "uCounts.reverse()\n",
    "\n",
    "bCounts = [(biCount[w], w) for w in biCount]\n",
    "bCounts.sort()\n",
    "bCounts.reverse()\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "#print(uCounts)\n",
    "#print(bCounts)\n",
    "\n",
    "#this has unigrams and bigrams:\n",
    "#print(counts)\n",
    "\n",
    "unigrams = [x[1] for x in uCounts[:1000]]\n",
    "bigrams = [x[1] for x in bCounts[:1000]]\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "#print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniId = dict(zip(unigrams, range(len(unigrams))))\n",
    "uniSet = set(unigrams)\n",
    "\n",
    "biId = dict(zip(bigrams, range(len(bigrams))))\n",
    "biSet = set(bigrams)\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#feature for using unigrams\n",
    "def feature1(datum):\n",
    "    feat = [0]*len(unigrams)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    for w in ws:\n",
    "        if w in unigrams:\n",
    "            feat[uniId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "\n",
    "#feature for using bigrams\n",
    "def feature2(datum):\n",
    "    feat = [0]*len(bigrams)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w in ws2:\n",
    "        if w in bigrams:\n",
    "            feat[biId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "\n",
    "#feature for using the mix of unigrams and bigrams\n",
    "def feature3(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w in ws + ws2:\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_a = [feature1(d) for d in trainSet]\n",
    "Xtrain_b = [feature2(d) for d in trainSet]\n",
    "Xtrain_c = [feature3(d) for d in trainSet]\n",
    "\n",
    "y_train = [d['rating'] for d in trainSet]\n",
    "\n",
    "Xtest_a = [feature1(d) for d in testSet]\n",
    "Xtest_b = [feature2(d) for d in testSet]\n",
    "Xtest_c = [feature3(d) for d in testSet]\n",
    "\n",
    "y_test = [d['rating'] for d in testSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_a = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf_a.fit(Xtrain_a, y_train)\n",
    "theta_a = clf_a.coef_\n",
    "preds_a = clf_a.predict(Xtest_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_b = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf_b.fit(Xtrain_b, y_train)\n",
    "theta_b = clf_b.coef_\n",
    "preds_b = clf_b.predict(Xtest_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_c = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf_c.fit(Xtrain_c, y_train)\n",
    "theta_c = clf_c.coef_\n",
    "preds_c = clf_c.predict(Xtest_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniSort = list(zip(theta_a[:-1], unigrams))\n",
    "uniSort.sort()\n",
    "\n",
    "biSort = list(zip(theta_b[:-1], bigrams))\n",
    "biSort.sort()\n",
    "\n",
    "wordSort = list(zip(theta_c[:-1], words))\n",
    "wordSort.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2271716427139745\n",
      "[1.2271716427139745, ['disappointing', 'boring', 'visuals', 'mangas', 'says'], ['5', 'yourself', 'serie', 'beautifully', 'wait']]\n",
      "1.2850254712593712\n",
      "[1.2850254712593712, ['tuned for', 'thoughts below', 'your next', 'the worst', 'a bad'], ['loved this', '5 stars', 'stay tuned', 'cant wait', 'forget to']]\n",
      "1.220884794454163\n",
      "[1.220884794454163, ['tuned for', 'as promos', 'miss your', 'xoxo katie', 'thoughts below'], ['due to', 'manga happy', 'tuned', 'xoxo', 'reviews as']]\n"
     ]
    }
   ],
   "source": [
    "for q in 'Q1a', 'Q1b', 'Q1c':\n",
    "    if(q == 'Q1a'):\n",
    "        mse = MSE(preds_a, y_test)\n",
    "        answers[q] = [float(mse), [x[1] for x in uniSort[:5]], [x[1] for x in uniSort[-5:]]]\n",
    "        print(mse)\n",
    "        print(answers[q])\n",
    "    elif(q == 'Q1b'):\n",
    "        mse = MSE(preds_b, y_test)\n",
    "        answers[q] = [float(mse), [x[1] for x in biSort[:5]], [x[1] for x in biSort[-5:]]]\n",
    "        print(mse)\n",
    "        print(answers[q])\n",
    "    else:\n",
    "        mse = MSE(preds_c, y_test)\n",
    "        answers[q] = [float(mse), [x[1] for x in wordSort[:5]], [x[1] for x in wordSort[-5:]]]\n",
    "        print(mse)\n",
    "        print(answers[q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in 'Q1a', 'Q1b', 'Q1c':\n",
    "    assert len(answers[q]) == 3\n",
    "    assertFloat(answers[q][0])\n",
    "    assert [type(x) for x in answers[q][1]] == [str]*5\n",
    "    assert [type(x) for x in answers[q][2]] == [str]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount2 = defaultdict(int)\n",
    "for review in trainReviewText:\n",
    "    r = ''.join([c for c in review.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount2[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts2 = [(wordCount2[w], w) for w in wordCount2]\n",
    "counts2.sort()\n",
    "counts2.reverse()\n",
    "\n",
    "words2 = [x[1] for x in counts2[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = defaultdict(int)\n",
    "for review in trainReviewText:\n",
    "    r = ''.join([c for c in review.lower() if not c in punctuation])\n",
    "    for w in set(r.split()):\n",
    "        df[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"compared to thw first review in the dataset\"\n",
    "rev = trainReviewText[0]\n",
    "#print(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = defaultdict(int)\n",
    "r = ''.join([c for c in rev.lower() if not c in punctuation])\n",
    "for w in r.split():\n",
    "    # Note = rather than +=, different versions of tf could be used instead\n",
    "    tf[w] = 1\n",
    "    \n",
    "tfidf = dict(zip(words2,[tf[w] * math.log2(len(trainReviewText) / df[w]) for w in words2]))\n",
    "tfidfQuery = [tf[w] * math.log2(len(trainReviewText) / df[w]) for w in words2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTf = [(tf[w],w) for w in words2]\n",
    "maxTf.sort(reverse=True)\n",
    "maxTfIdf = [(tfidf[w],w) for w in words2]\n",
    "maxTfIdf.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity:\n",
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for a1,a2 in zip(x1,x2):\n",
    "        numer += a1*a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1*norm2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for rev2 in trainReviewText:\n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in rev2.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        # Note = rather than +=\n",
    "        tf[w] = 1\n",
    "    tfidf2 = [tf[w] * math.log2(len(trainReviewText) / df[w]) for w in words2]\n",
    "    similarities.append((Cosine(tfidfQuery, tfidf2), rev2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.sort(reverse=True)\n",
    "#print(similarities[0][1])\n",
    "sim = similarities[0][0]\n",
    "review = similarities[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [sim, review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q2']) == 2\n",
    "assertFloat(answers['Q2'][0])\n",
    "assert type(answers['Q2'][1]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    reviewsPerUser[d['user_id']].append((dateutil.parser.parse(d['date_added']), d['book_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewLists = []\n",
    "for u in reviewsPerUser:\n",
    "    rl = list(reviewsPerUser[u])\n",
    "    rl.sort()\n",
    "    reviewLists.append([x[1] for x in rl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = Word2Vec(reviewLists,\n",
    "                 min_count=1, # Words/items with fewer instances are discarded\n",
    "                 vector_size=10, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=1) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('8497638', 0.943289577960968), ('25032624', 0.9344254732131958), ('21519210', 0.8979508280754089), ('22752448', 0.8910755515098572), ('5497136', 0.8672892451286316)]\n"
     ]
    }
   ],
   "source": [
    "sims = model10.wv.similar_by_word('18471619')\n",
    "similarities = sims[:5]\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = similarities # probably want model10.wv.similar_by_word(...)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q3']) == 5\n",
    "assert [type(x[0]) for x in answers['Q3']] == [str]*5\n",
    "assertFloatList([x[1] for x in answers['Q3']], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02713349 -0.01818817 -0.06850455 -0.09552109 -0.0706069  -0.05654795\n",
      "  0.09549883  0.00884362 -0.04371066 -0.00882185]\n"
     ]
    }
   ],
   "source": [
    "print(model10.wv['18471619']) #numpy vector of a book id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset[0])\n",
    "#user_id\n",
    "#book_id\n",
    "#review_id\n",
    "#rating\n",
    "#review_text\n",
    "#date_added\n",
    "#date_updated\n",
    "#read_at\n",
    "#started_at\n",
    "#n_votes\n",
    "#n_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "reviewsPerUser = defaultdict(list) # Maps a user to the reviews they made\n",
    "reviewsPerItem = defaultdict(list) # Maps an item to its reviews\n",
    "ratingDict = {} # To retrieve a rating for a specific user/item pair\n",
    "\n",
    "for d in trainSet:\n",
    "    user,item = d['user_id'], d['book_id']\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)\n",
    "    ratingDict[(user,item)] = d['rating']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for d in dataset]) / len(dataset)\n",
    "\n",
    "itemAverages = {}\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages[i] = sum(rs) / len(rs)\n",
    "\n",
    "\n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    simscores = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        #print(d)\n",
    "        j = d['book_id'] #you're getting the id of the current book that user read\n",
    "        if j == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[j])\n",
    "        simscores.append(Cosine(model10.wv[item],model10.wv[j]))\n",
    "    if (sum(simscores) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,simscores)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(simscores)\n",
    "    else:\n",
    "        return ratingMean\n",
    "\n",
    "\n",
    "simPredictions = [predictRating(d['user_id'], d['book_id']) for d in dataset[:1000]]\n",
    "labels = [d['rating'] for d in dataset[:1000]]\n",
    "\n",
    "mse4 = MSE(simPredictions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225.11155800094284\n"
     ]
    }
   ],
   "source": [
    "print(mse4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = mse4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Word2Vec(reviewLists,\n",
    "                 min_count=1, # Words/items with fewer instances are discarded\n",
    "                 vector_size=8, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=1) # Skip-gram model\n",
    "\n",
    "ratingMean2 = sum([d['rating'] for d in dataset]) / len(dataset)\n",
    "\n",
    "itemAverages2 = {}\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    rs = [ratingDict[(u,i)] for u in usersPerItem[i]]\n",
    "    itemAverages2[i] = sum(rs) / len(rs)\n",
    "\n",
    "\n",
    "def predictRating2(user,item):\n",
    "    ratings2 = []\n",
    "    simscores2 = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        #print(d)\n",
    "        j = d['book_id'] #you're getting the id of the current book that user read\n",
    "        if j == item: continue\n",
    "        ratings2.append(d['rating'] - itemAverages2[j])\n",
    "        simscores2.append(Cosine(model5.wv[item],model5.wv[j]))\n",
    "    if (sum(simscores2) > 0):\n",
    "        weightedRatings2 = [(x*y) for x,y in zip(ratings2,simscores2)]\n",
    "        return itemAverages[item] + sum(weightedRatings2) / sum(simscores2)\n",
    "    else:\n",
    "        return ratingMean\n",
    "\n",
    "\n",
    "simPredictions2 = [predictRating2(d['user_id'], d['book_id']) for d in dataset[:1000]]\n",
    "labels2 = [d['rating'] for d in dataset[:1000]]\n",
    "\n",
    "mse5 = MSE(simPredictions2,labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.638377985784956\n"
     ]
    }
   ],
   "source": [
    "print(mse5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = [\"For my solution, I changed the vector size from 10 to 8\",\n",
    "                 mse5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q5']) == 2\n",
    "assert type(answers['Q5'][0]) == str\n",
    "assertFloat(answers['Q5'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw4.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d1f2b9dc6ac590fb1d3578e7edd064d59472f2d86a6b128e389e381c4d0910c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
